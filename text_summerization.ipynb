{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text  summerization using Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than having to goin ahead and train huge language model we are actually able to leverage a prebuilt summarization pipeline that hugging face has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install transformers and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.7.24-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.4-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\teena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Using cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Downloading regex-2024.7.24-cp312-cp312-win_amd64.whl (269 kB)\n",
      "Downloading safetensors-0.4.4-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 2.1/2.2 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 12.5 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.24.6 regex-2024.7.24 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline #a pipeline gives us the method to easily download and use summerization pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load summarization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0e0180bfac492193e2fb89a71c10d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\teena\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e096e1f842144c193d8b2a60fbdffd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\teena\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29aa39c17e34cbbbe2abd2536626d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0f6a55bd614557b0a7594010b75e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe853d6c64654675b606e7329a1f3d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer=pipeline('summarization')#this tells hugging face that we want to use pretrained summarization pipeline and brings it into the note book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article='''\n",
    "Finding success in entrepreneurship is hard. I would know. I have spent many years talking about, or being involved in, entrepreneurship. I started a UK-based fashion business six years ago, and that all-encompassing dumpster fire makes up most of what I know today about marketing, e-commerce, branding, and how no one will buy your stuff if you just do things the way they’ve always been done.\n",
    "\n",
    "Four years later, I got back onto the anthropomorphic entrepreneurship horse and tried to start a German-based social business, which had the goal of providing affordable and safe housing to German sex workers. That too was a failure. Turns out it’s hard to get people to care about people who have sex for living.\n",
    "\n",
    "In addition to these half-baked experiments, I followed a handful of entrepreneurship classes at the University of Exeter (England), Universität Mannheim (Germany), and ESSEC Business School (France). Beyond what I’d learned practically, these courses gave me what I’d call a blanket theoretical knowledge of what it takes to call oneself an entrepreneur (big surprise, I don’t have what it takes).\n",
    "\n",
    "I only describe this to highlight the fact that the following few paragraphs are the result of careful consideration. It’s not that I’m anti-innovation or a disbeliever in disruption (or whatever another aggravating cliché the start-up community may use), or even a regulation hawk (but companies should be held accountable for the harm they may cause, sue me) : I simply think that entrepreneurship is rotten to its very core and that one way to fix it is to change some of the things we teach about it in business schools.\n",
    "\n",
    "It wasn’t always this way. Things were going pretty smoothly on the good start-up ship ten years ago, but ever since the captain/government gave the keys to the passengers/founders, there are people passed out everywhere, rowdy guests are throwing empty beer cans at the neighbor’s garden and the furniture is getting trashed. Things got so out of hand that the cops won’t even show up because half the guest have enough money to make anyone’s and everyone’s life a living, dystopian nightmare (looking at you, Facebook, Amazon, Google…).\n",
    "\n",
    "Fun metaphor aside, I believe three things should be explained to young ones to prepare them for modern entrepreneurship: Safety Nets, Shortcuts and The Five.\n",
    "\n",
    "Safety Nets\n",
    "Students in virtually every higher education classroom throughout the world are told that they can make it as an entrepreneur with only a great team and an astounding idea (hell, for some courses, the entire curriculum revolves around that idea). That’s, however, turning a blind eye to the most common shared trait among entrepreneurs: access to financial capital (family money, inheritance, or connections that allow for access to financial stability).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=summarizer(article,max_length=130,min_length=30,do_sample=False)\n",
    "#do_sample=False:this tells us the summarizer that we want to use a greedy decoder,what this actually means is that we are gonna return back each word.i.e when we are returning the sequence we are returning back the next word which has the highest probabilty of actually making sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entrepreneurship is rotten to its very core and that one way to fix it is to change some of the things we teach about it in business schools . safety nets, shortcuts and The Five should be explained to young ones to prepare them for modern entrepreneurship .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text[0]['summary_text'].split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9de1db7c63fc4d38f1de0a13c1ade948eddb9f1eb885da925c1ad3252bfea3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
